{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np \n","import torch\n","import matplotlib.pyplot as plt\n","import dataSource as ds\n",""]},{"cell_type":"markdown","metadata":{},"source":["# Parametrage"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","df = ds.importData()\n","\n","def treat(df, batch_size, train_size, step):\n","\n","    df = df.sort_values(by=\"GpsTime\")\n","    \n","    df = df[df[\"Trip\"] == 57]\n","\n","    data = df[[\"Latitude\",\"Longitude\"]]\n","    \n","    step = step//200\n","\n","    N = len(data)\n","    train_df, test_df = data[:int(N*train_size)], data[int(N*train_size):]\n","\n","    X_train = ds.echantillon(train_df[:-step], step).to_numpy()\n","    y_train = ds.echantillon(train_df[step:], step).to_numpy()\n","\n","    X_test = ds.echantillon(test_df[:-step], step).to_numpy()\n","    y_test = ds.echantillon(test_df[step:], step).to_numpy()\n","\n","    return X_train, y_train, X_test, y_test\n","\n","batch_size,train_size,step = 32, 0.8, 200\n","\n","X_train, y_train, X_test, y_test = treat(df, batch_size, train_size, step)\n","\n","plt.scatter(X_train[:, 0], X_train[:, 1], s = 1, c = 'y')\n","plt.scatter(y_train[:, 0], y_train[:, 1], s = 1, c = 'r')\n","plt.show()\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#dataset\n","from torch.utils.data import Dataset\n","\n","class timeseries(Dataset):\n","    def __init__(self,x,y):\n","        self.x = torch.tensor(x,dtype=torch.float32)\n","        self.y = torch.tensor(y,dtype=torch.float32)\n","        self.len = x.shape[0]\n","\n","    def __getitem__(self,idx):\n","        return self.x[idx],self.y[idx]\n","  \n","    def __len__(self):\n","        return self.len\n","\n","dataset = timeseries(X_train,y_train)\n","#dataloader\n","from torch.utils.data import DataLoader \n","train_loader = DataLoader(dataset,shuffle=False,batch_size=batch_size, drop_last = True)\n","\n","# verify train_loader, especially check if batches are all consecutives\n","'''\n","torch.set_printoptions(sci_mode=False)\n","l = list(train_loader)\n","print(l[0], l[1])\n","'''\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# rnn\n","#neural network\n","from torch import nn\n","\n","class Model(nn.Module):\n","    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n","        super(Model, self).__init__()\n","\n","        # Defining some parameters\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","\n","        #Defining the layers\n","        # RNN Layer\n","        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers)   \n","        # Fully connected layer\n","        self.fc = nn.Linear(hidden_dim, output_size)\n","    \n","    def forward(self, x, state):\n","        \"\"\"As we divided data into batches, to respect time order, every forward should take previous state and, produces a new state\"\"\"\n","\n","        #Initializing hidden state for first input using method defined below\n","        #hidden = self.init_hidden(batch_size)\n","\n","        # Passing in the input and hidden state into the model and obtaining outputs\n","        out, new_state = self.lstm(x, state)\n","        \n","        # Reshaping the outputs such that it can be fit into the fully connected layer\n","        out = out.contiguous().view(-1, self.hidden_dim)\n","        out = self.fc(out)\n","        \n","        return out, new_state\n","    \n","    def init_hidden(self, batch_size):\n","        # This method generates the first hidden state of zeros which we'll use in the forward pass\n","         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n","        hidden = ( torch.zeros((self.n_layers, batch_size, self.hidden_dim)), torch.zeros((self.n_layers, batch_size, self.hidden_dim)) )\n","        return hidden\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Instantiate the model with hyperparameters\n","\n","device = torch.device(\"cpu\")\n","\n","input_size = output_size = 2 # len([\"Latitude\",\"Longitude\"])\n","# Define hyperparameters\n","\n","lr = 1e-2\n","num_hiddens = 256 # n. hidden units\n","num_layer = 1   # 1 single rnn_layer\n","                # multiple layers are possible but we start with a single one..\n","\n","model = Model(input_size=input_size, output_size=output_size, hidden_dim=num_hiddens, n_layers=num_layer)\n","\n","# We'll also set the model to the device that we defined earlier (default is CPU)\n","model = model.to(device)\n","\n","\n","# Define Loss, Optimizer\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# raw output\n","#init_state = model.init_hidden(batch_size)\n","#model(list(train_loader)[0], init_state)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# training \n","\n","def grad_clipping(net, theta): \n","    if isinstance(net, nn.Module):\n","        params = [p for p in net.parameters() if p.requires_grad]\n","    else:\n","        params = net.params\n","    norm = torch.sqrt(sum(torch.sum((p.grad**2)) for p in params))\n","    if norm > theta:\n","        for param in params:\n","            param.grad[:] *= theta / norm\n","\n","\n","epochs = 500\n","\n","state = model.init_hidden(batch_size) # init state, dim (n_layers, batch_size, hidden_dim)\n","\n","for i in range(epochs):\n","    for j, data in enumerate(train_loader):\n","        #print(data.shape, data)\n","        #print(data[0].view(1, batch_size, input_size).shape)\n","\n","        y_pred, state = model(data[0].view(1, batch_size, input_size), state)\n","        \n","        loss = criterion(y_pred,data[1])\n","        \n","        for s in state:\n","            s.detach_()\n","\n","        optimizer.zero_grad()\n","        loss.backward() # each successive batch will take more time than the previous one because it will have to back-propagate all the way through to the start of the first batch.\n","        grad_clipping(model, 1)\n","        optimizer.step()\n","    if i%50 == 0:\n","        print(i,\"th iteration : \",loss)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# test set actual vs predicted\n","\n","def pred(input, num_future, model):\n","    \"\"\"\n","    take last output as next input; init hidden state = zeros ; take last hidden state as next hidden state.\n","    \n","    input : prefix data points (X_test when scoring)\n","    num_future : number of points we want to predict, (0 when scoring)\n","    \n","    \"\"\"\n","    batch_size=1\n","    \n","    state = model.init_hidden(batch_size=batch_size) \n","\n","    output = [] # first point\n","\n","    for i in range(len(input) + num_future):\n","        if i == 0:\n","            yhat, state = model(input[0].view(-1, batch_size, 2), state)    \n","        else:\n","            yhat, state = model(output[-1].view(-1, batch_size, 2), state)\n","        #print(yhat, output[-1])\n","        if i < len(input) - 1:\n","            output.append(input[i+1])\n","        else:\n","            output.append(yhat.detach())\n","\n","    return output\n","\n","output = pred(torch.Tensor([[-1.5134,  1.3806]]), 10, model)\n","plt.plot([e[0][0].item() for e in output], [e[0][1].item() for e in output], label='predicted{}'.format((j)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","test_set = timeseries(X_test,y_test)\n","#test_loader = DataLoader(test_set,shuffle=False,batch_size=batch_size, drop_last = True)\n","\n","#state_pred = state\n","\n","plt.scatter(test_set.y[:,0],test_set.y[:,1], s = 1,label='original')\n","\n","for j, data in enumerate(test_set.x):\n","    \n","    y_pred, state_pred = model(data[0].view(1,batch_size,input_size), state_pred) # state was well trained after iteration\n","    \n","    tmp = y_pred.detach().numpy()\n","    \n","    plt.scatter(tmp[:,0], tmp[:,1], s = 1, label='predicted{}'.format((j)))\n","\n","\n","    plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}